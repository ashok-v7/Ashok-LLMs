{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"text-embedding-3-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir=\"updatedFolder/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(dir):\n",
    "    loader = DirectoryLoader(dir,loader_cls=PyPDFLoader,use_multithreading=True,max_concurrency=120,show_progress=True,silent_errors=True)\n",
    "    documents = loader.load()\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the documents into chunks\n",
    "\n",
    "def split_docs(documents,chunk_size=1000,chunk_overlap=100):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "    docs=text_splitter.split_documents(documents)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=load_docs(dir)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "doc=split_docs(documents)\n",
    "print(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#langchain client OpenAIEmbeddings\n",
    "\n",
    "clientopen = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    base_url=endpoint,\n",
    "    api_key=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to=Chroma.from_documents(documents=doc,embedding=clientopen,persist_directory='./updated-News')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Gemini 2.5 Pro is currently avaialble in Google AI Studio?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357315/3330713543.py:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db1=Chroma(persist_directory='./updated-News',embedding_function=clientopen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'creationdate': '2025-03-26T15:52:00+05:30', 'creator': 'PyPDF', 'page': 0, 'page_label': '1', 'producer': 'cairo 1.16.0 (https://cairographics.org)', 'source': 'updatedFolder/gg-tkt.pdf', 'total_pages': 1}, page_content='Gemini 2.5 builds on what makes Gemini models great — native multimodality and a long\\ncontext window. 2.5 Pro ships today with a 1 million token context window (2 million\\ncoming soon), with strong performance that improves over previous generations. It can\\ncomprehend vast datasets and handle complex problems from different information sources,\\nincluding text, audio, images, video and even entire code repositories.\\n \\nDevelopers and enterprises can start experimenting with Gemini 2.5 Pro in Google AI Studio\\nnow, and Gemini Advanced users can select it in the model dropdown on desktop and mobile.\\nIt will be available on Vertex AI in the coming weeks.'), Document(metadata={'creationdate': '2025-03-26T15:52:00+05:30', 'creator': 'PyPDF', 'page': 0, 'page_label': '1', 'producer': 'cairo 1.16.0 (https://cairographics.org)', 'source': 'updatedFolder/gg-tkt.pdf', 'total_pages': 1}, page_content='Gemini 2.5 builds on what makes Gemini models great — native multimodality and a long\\ncontext window. 2.5 Pro ships today with a 1 million token context window (2 million\\ncoming soon), with strong performance that improves over previous generations. It can\\ncomprehend vast datasets and handle complex problems from different information sources,\\nincluding text, audio, images, video and even entire code repositories.\\n \\nDevelopers and enterprises can start experimenting with Gemini 2.5 Pro in Google AI Studio\\nnow, and Gemini Advanced users can select it in the model dropdown on desktop and mobile.\\nIt will be available on Vertex AI in the coming weeks.'), Document(metadata={'creationdate': '2025-03-26T15:52:00+05:30', 'creator': 'PyPDF', 'page': 0, 'page_label': '1', 'producer': 'cairo 1.16.0 (https://cairographics.org)', 'source': 'updatedFolder/gg-tkt.pdf', 'total_pages': 1}, page_content='File: Untitled Document 2 Page 1 of 1\\n \\n \\nGemini 2.5 Pro Experimental is our most advanced model for complex tasks. It tops the\\nLMArena leaderboard — which measures human preferences — by a significant margin,\\nindicating a highly capable model equipped with high-quality style. 2.5 Pro also shows\\nstrong reasoning and code capabilities, leading on common coding, math and science\\nbenchmarks.\\n \\nGemini 2.5 Pro is available now in Google AI Studio and in the Gemini app for Gemini\\nAdvanced users, and will be coming to Vertex AI soon. We’ll also introduce pricing in the\\ncoming weeks, enabling people to use 2.5 Pro with higher rate limits for scaled production\\nuse.\\n \\nEnhanced reasoning\\nGemini 2.5 Pro is state-of-the-art across a range of benchmarks requiring advanced\\nreasoning. Without test-time techniques that increase cost, like majority voting, 2.5 Pro\\nleads in math and science benchmarks like GPQA and AIME 2025.'), Document(metadata={'creationdate': '2025-03-26T15:52:00+05:30', 'creator': 'PyPDF', 'page': 0, 'page_label': '1', 'producer': 'cairo 1.16.0 (https://cairographics.org)', 'source': 'updatedFolder/gg-tkt.pdf', 'total_pages': 1}, page_content='File: Untitled Document 2 Page 1 of 1\\n \\n \\nGemini 2.5 Pro Experimental is our most advanced model for complex tasks. It tops the\\nLMArena leaderboard — which measures human preferences — by a significant margin,\\nindicating a highly capable model equipped with high-quality style. 2.5 Pro also shows\\nstrong reasoning and code capabilities, leading on common coding, math and science\\nbenchmarks.\\n \\nGemini 2.5 Pro is available now in Google AI Studio and in the Gemini app for Gemini\\nAdvanced users, and will be coming to Vertex AI soon. We’ll also introduce pricing in the\\ncoming weeks, enabling people to use 2.5 Pro with higher rate limits for scaled production\\nuse.\\n \\nEnhanced reasoning\\nGemini 2.5 Pro is state-of-the-art across a range of benchmarks requiring advanced\\nreasoning. Without test-time techniques that increase cost, like majority voting, 2.5 Pro\\nleads in math and science benchmarks like GPQA and AIME 2025.')]\n",
      "Gemini 2.5 builds on what makes Gemini models great — native multimodality and a long\n",
      "context window. 2.5 Pro ships today with a 1 million token context window (2 million\n",
      "coming soon), with strong performance that improves over previous generations. It can\n",
      "comprehend vast datasets and handle complex problems from different information sources,\n",
      "including text, audio, images, video and even entire code repositories.\n",
      " \n",
      "Developers and enterprises can start experimenting with Gemini 2.5 Pro in Google AI Studio\n",
      "now, and Gemini Advanced users can select it in the model dropdown on desktop and mobile.\n",
      "It will be available on Vertex AI in the coming weeks.\n"
     ]
    }
   ],
   "source": [
    "db1=Chroma(persist_directory='./updated-News',embedding_function=clientopen)\n",
    "results=db1.similarity_search(query)\n",
    "print(results)\n",
    "print(results[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
